<p align=center><img src=https://github.com/espadaone/PF_UCI-/blob/d001d3245a72a4123a2a2b0d0a54eef342f06d47/images/Header.jpg><p>

# <h1 align=center> **DATA SCIENCE** </h1>

# <h1 align=center>**`EDA, ETL, ML y Dashboard de Datos de UCI`**</h1>

<hr>  

¬°A continuaci√≥n se describir√° los procesos de Extract, transform and load - ETL, Exploratory Data Analysis - EDA, Machine Learning, Data WareHouse y Data Analytics llevados a cabo en el presente proyecto de ***Data Science*** para el √°rea de ***UCI**** de un Hospital.

<hr>

## Contexto

XXXX

<hr>

## Objetivos

+ Facilitar el entendimiento de los datos, sobre el paciente y  los recursos
+ Mejorar la capacidad de toma de decisiones
+ Aplicar nuevas tecnolog√≠as al negocio

<hr>

## Alcance

`¬øQu√© S√ç incluye?`

+ Procesamiento y almacenamiento.
+ Generaci√≥n de ratios e indicadores
+ Visualizaci√≥n de datos
+ Estimar tasas
 
`¬øQu√© S√ç incluye?`

+ Sugerencias de medicaci√≥n
+ Administrar los recursos
+ Predicciones de supervivencia / mortalidad
+ Disponibilidad de recursos
+ Clasificaci√≥n de pacientes

<hr>

## **I. MODELO ENTIDAD RELACI√ìN**

El modelo ER de este proyecto se arm√≥ seg√∫n las columnas claves de cada tabla y evitando la redundancia de las referencias. A m√°s detalle se puede observar en la siguiente refresenteaci√≥n:

<p align=center><img src=images/xxx.png><p>

`VER MODELO ENTIDAD RELACI√ìN COMPLETO` üëá

[![VER](images/C.jpg)](https://github.com/espadaone/PF_UCI-/blob/bd0624f0bc79de89466ceb253129db5476031245/Diccionario_datos_actualizado.pdf)



<br/>

# **III. DATA WAREHOUSE**

Para asegurar el √©xito de este proyecto se ha utilizado diferentes herramientas tecnol√≥gicas que permiten ejecutar de la mejor manera el proyecto. Se ha empleado un conjunto de tecnolog√≠as que abarcan diferentes √°reas. En cuanto a la gesti√≥n y comunicaci√≥n, se han utilizado herramientas como Slack, Google Apps y Github. Para el desarrollo, se ha hecho uso de lenguajes de programaci√≥n como Python, junto con librer√≠as como Pandas y NumPy, y se han utilizado bases de datos como PostgreSQL y SQL Azure, alojado en la plataforma Microsoft Azure. En cuanto a la visualizaci√≥n de los resultados obtenidos, se han utilizado herramientas como Matplotlib, Streamlit y Power BI. Finalmente, para la creaci√≥n de modelos de machine learning, se ha hecho uso de la popular librer√≠a SKlearn.

A continuaci√≥n les mostramos un resumen del tecnol√≥gico:

<p align=center><img src=images/stack_tecnologico.png><p>

Las herramientas m√°s importantes para el desarrollo y an√°lisis de datos de este proyecto son los siguientes. Microsoft Azure, Python, Power BI, Github y Slack. Microsoft Azure ofrece un entorno virtual que permite la escalabilidad de datos, mayor velocidad de procesamiento, reducci√≥n de costo de mantenimiento de servidor local, mayor seguridad y protecci√≥n de datos. Python es esencial para el an√°lisis exploratorio de datos, el proceso de Extracci√≥n, Transformaci√≥n y Carga de datos, y la automatizaci√≥n de generaci√≥n de archivos csv para que luego est√©n disponibles en la m√°quina virtual de Azure. Power BI es un servicio de an√°lisis de datos de Microsoft que se enfoca en proporcionar visualizaciones interactivas y capacidades de inteligencia empresarial. Por otro lado, Github funciona como el repositorio del desarrollo del proyecto y control de versiones, mientras que Slack es el sistema de comunicaci√≥n continua del equipo de trabajo.

<br/>

# **VI. PIPELINE Y WORKFLOW DE LA INFORMACI√ìN**

Se cuenta con una fuente de datos que est√° compuesta por un total de 26 archivos `csv`, los cuales son cargados a un archivo de Python. Para la realizaci√≥n del proceso ETL, se utiliza la `librer√≠a de pandas` y una vez llevadas a cabo las transformaciones, se procede a validar los datos y comenzar con la carga de la informaci√≥n. Dicha carga se efect√∫a mediante la `librer√≠a de pymysql`, la cual se encarga de generar la conexi√≥n con la base de datos. Las tablas requeridas ya se encuentran cargadas en la base de datos y son llenadas conforme se ejecuta la carga, incluyendo la generaci√≥n de las relaciones necesarias. Una vez terminado el proceso, la base de datos queda lista para ser subida a la `nube de Azure`. La automatizaci√≥n de ETL se realiza con `airflow` y se ejecuta diariamente a las 6 de la ma√±ana, conect√°ndose directamente con Azure. Por √∫ltimo, la data obtenida es consultada y consumida por los procesos de `visualizaci√≥n` y el modelo de `Machine Learning`.

<p align=center><img src=images/Pipeline.png><p>

<br/>

# **V. ETL**

## **1. Extracci√≥n**

La informaci√≥n es recopilada desde archivos csv que son proporcionados por el hospital. Estos archivos son luego cargados en un archivo `.py` con la ayuda de la librer√≠a `pandas`, y se almacenan en un dataframe para su posterior transformaci√≥n y carga.

## **2. Transformaci√≥n**

Una vez extraida la data se realizaron las siguientes transformaciones:

**`1. Cambiar formato de las columnas de fechas`**

Las columnas que incluian fechas se cambi√≥ el formato de texto a datetime.

`2. Cambio de formato de medidas`

Una de las unidades de medida m√°s vista en los datos fue ‚Äúml‚Äù y ‚ÄúmL‚Äù, significan lo mismo(mililitros), as√≠ que todo se transformo a ‚ÄúmL‚Äù, esto solo aplica a las unidades de medida escalares y no a las vectoriales.

`3. Reemplazar Nulos`

Todos los valores N\D =  No data (Para valores nulos en campos de texto) fueron reemplazados por "0‚Äù o promedios en caso de columnas num√©ricas.

Los campos donde el tipo de dato es datetime se dej√≥ con nulos debido a que es en muchos casos completarlo con otros valores generan errores de registros.

`4. Eliminaci√≥n de tablas`

La tabla 'NOTEEVENTS' no fue incluida en la base de datos ya que no contiene ning√∫n dato.

`5. Replanteamiento de tablas`

Se elimina las columnas ('insurance','language', 'religion', 'marital_status', 'ethnicity') de la tabla ADMISSIONS y fueron colocadas en la tabla PATIENTS. Se lleg√≥ a la conclusi√≥n de que las columnas eliminadas estar√≠an mejor introducidas en la tabla de pacientes.

`6. Reordenamiento de los nombres de las columnas`

Se estableci√≥ un nuevo √≥rden de las columnas colocando la columna PK en la primera columna.

`7. Eliminaci√≥n de columnas`

La columna ‚Äúrow_id‚Äù se elimin√≥ de las tablas donde ya se contaba con PK y donde no se contaba se constituy√≥ en la PK

`8. Se eliminan duplicados`

Se eliminan duplicados de las tablas que contaban con duplicados.


## **3. Carga**

El c√≥digo ETL creado transforma cada dataset en una sentencia T-SQL (INSERT INTO‚Ä¶) para agregar los datos a las tablas en SQL en un solo movimiento. Para lograrlo se ha usado la siguiente librer√≠a:

    import pymysql

`Creaci√≥n de Data Warehouse`

Como DBMS se utiliz√≥ MySQL workbench, todas las tablas fueron creadas utilizando T-SQL.

`Creaci√≥n manual de BD en MySQL`

La BD lleva por nombre pf_uci  y est√° creada dentro de Mysql Workbench.

`Creaci√≥n de tablas en MySQL`

La base de datos est√° conformada por 25 tablas. Cada tabla lleva los mismos nombres que los csv que fueron disponibilizados por el hospital. La tabla NOTEEVENTS no fue incluida en el armado de la base de datos ya que no es necesaria, se recomienda al cliente que deje de tomarla en cuenta o haga una modificaci√≥n en los datos que est√° recabando. Se renombraron las columnas con el nombre row_id, el nuevo formato de nombre para esas columnas es el nombre correspondiente de la tabla m√°s ( ‚Äú_id‚Äù) en las tablas de hechos. Esta columna fue removida de las tablas de dimensiones.

`Asignaci√≥n de PK y FK`

Una vez que los datos se han transformado, se pueden cargar en un destino deseado, como un archivo CSV o una base de datos. Las sentencias son construidas respetando las PK y FK, adem√°s de los formatos de las tablas.

`Automatizaci√≥n de tareas con AirFlow`

Finalmente, se ha automatizado el proceso ETL utilizando un programador de tareas o una herramienta de orquestaci√≥n como Apache Airflow para ejecutar el pipeline ETL en intervalos regulares.


  <br/>

# **VI. KPIs**
  
  <br/>

# **VII. MODELO DE MACHINE LEARNING**

<br/>

<p align=center><img src=https://github.com/espadaone/PF_UCI-/blob/c31c4bbdec1a6725a0daae09bac0b100a64608da/images/banner%20inferior.png><p>

<br/>

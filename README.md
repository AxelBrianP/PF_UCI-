<p align=center><img src=https://github.com/espadaone/PF_UCI-/blob/d001d3245a72a4123a2a2b0d0a54eef342f06d47/images/Header.jpg><p>

# <h1 align=center> **DATA SCIENCE** </h1>

# <h1 align=center>**`EDA, ETL, ML y Dashboard de Datos de UCI`**</h1>

<hr>  

¬°A continuaci√≥n se describir√° los procesos de Extract, transform and load - ETL, Exploratory Data Analysis - EDA, Machine Learning, Data WareHouse y Data Analytics llevados a cabo en el presente proyecto de ***Data Science*** para el √°rea de ***UCI**** de un Hospital.

<hr>

## Contexto
XXXX

<hr>

## Objetivos

+ Facilitar el entendimiento de los datos, sobre el paciente y  los recursos
+ Mejorar la capacidad de toma de decisiones
+ Aplicar nuevas tecnolog√≠as al negocio

<hr>
## Objetivos

`¬øQu√© S√ç incluye?`

+ Procesamiento y almacenamiento.
+ Generaci√≥n de ratios e indicadores
+ Visualizaci√≥n de datos
+ Estimar tasas
 
`¬øQu√© S√ç incluye?`

+ Sugerencias de medicaci√≥n
+ Administrar los recursos
+ Predicciones de supervivencia / mortalidad
+ Disponibilidad de recursos
+ Clasificaci√≥n de pacientes

<hr>

## **I. MODELO ENTIDAD RELACI√ìN**

El modelo ER de este proyecto se arm√≥ seg√∫n las columnas claves de cada tabla y evitando la redundancia de las referencias. A m√°s detalle se puede observar en la siguiente refresenteaci√≥n:

<p align=center><img src=images/stack_tecnologico.png><p>

`VER MODELO ENTIDAD RELACI√ìN COMPLETO` üëá

[![VER](images/C.jpg)](https://github.com/espadaone/PF_UCI-/blob/bd0624f0bc79de89466ceb253129db5476031245/Diccionario_datos_actualizado.pdf)



<br/>

# **III. DATA WAREHOUSE**

Para asegurar el √©xito de este proyecto se ha utilizado diferentes herramientas tecnol√≥gicas que permiten ejecutar de la mejor manera el proyecto. Se ha empleado un conjunto de tecnolog√≠as que abarcan diferentes √°reas. En cuanto a la gesti√≥n y comunicaci√≥n, se han utilizado herramientas como Slack, Google Apps y Github. Para el desarrollo, se ha hecho uso de lenguajes de programaci√≥n como Python, junto con librer√≠as como Pandas y NumPy, y se han utilizado bases de datos como PostgreSQL y SQL Azure, alojado en la plataforma Microsoft Azure. En cuanto a la visualizaci√≥n de los resultados obtenidos, se han utilizado herramientas como Matplotlib, Streamlit y Power BI. Finalmente, para la creaci√≥n de modelos de machine learning, se ha hecho uso de la popular librer√≠a SKlearn.

A continuaci√≥n les mostramos un resumen del tecnol√≥gico:

<p align=center><img src=images/stack_tecnologico.png><p>

<br/>

# **VI. PIPELINE Y WORKFLOW DE LA INFORMACI√ìN**


<p align=center><img src=images/Pipeline.png><p>


<br/>

# **V. ETL**

## **1. Extracci√≥n**

Para gestionar adecuadamente la informaci√≥n de un hospital, es fundamental contar con un proceso de `ETL eficiente` y preciso que permita `extraer los datos de los csv` entregados por la instituci√≥n. En este sentido, la utilizaci√≥n de herramientas como Python y Airflow puede resultar muy √∫til para llevar a cabo este proceso de forma automatizada. Una vez realizada la extracci√≥n, transformaci√≥n y carga de los datos, es importante contar con un lugar seguro y confiable para almacenar la informaci√≥n, como puede ser una base de datos en la nube, como la `SQL de Azure`. De esta forma, se puede disponibilizar la informaci√≥n en tiempo real para los modelos de `predicci√≥n` y los `KPIs`. La informaci√≥n que se recolecta del hospital se obtiene de archivos csv que son proporcionados por el mismo. Luego, estos archivos son cargados a trav√©s de un archivo .py que utiliza la librer√≠a `pandas` para realizar las tareas de ETL. Para llevar a cabo la carga de la informaci√≥n en la base de datos se utiliza la librer√≠a `pymysql`, la cual establece la conexi√≥n necesaria desde el mismo archivo `.py` de ETL. El proceso completo de ETL se programa mediante `tareas con AirFlow` sobre la base de datos cada cierto tiempo, las cuales ser√°n actualizadas una vez al d√≠a a las 3 am. visualizaci√≥n en `Power BI`, lo que permitir√° tomar decisiones informadas y en tiempo y forma para mejorar la gesti√≥n del hospital.

## **2. Transformaci√≥n**

Una vez extraida la data se realizaron las siguientes transformaciones:

**`1. Cambiar formato de las columnas de fechas`**

Las columnas que incluian fechas se cambi√≥ el formato de texto a datetime.

`2. Cambio de formato de medidas`

Una de las unidades de medida m√°s vista en los datos fue ‚Äúml‚Äù y ‚ÄúmL‚Äù, significan lo mismo(mililitros), as√≠ que todo se transformo a ‚ÄúmL‚Äù, esto solo aplica a las unidades de medida escalares y no a las vectoriales.

`3. Reemplazar Nulos`

Todos los valores N\D =  No data (Para valores nulos en campos de texto) fueron reemplazados por "0‚Äù o promedios en caso de columnas num√©ricas.

Los campos donde el tipo de dato es datetime se dej√≥ con nulos debido a que es en muchos casos completarlo con otros valores generan errores de registros.

`4. Eliminaci√≥n de tablas`

La tabla 'NOTEEVENTS' no fue incluida en la base de datos ya que no contiene ning√∫n dato.

`5. Replanteamiento de tablas`

Se elimina las columnas ('insurance','language', 'religion', 'marital_status', 'ethnicity') de la tabla ADMISSIONS y fueron colocadas en la tabla PATIENTS. Se lleg√≥ a la conclusi√≥n de que las columnas eliminadas estar√≠an mejor introducidas en la tabla de pacientes.

`6. Reordenamiento de los nombres de las columnas`

Se estableci√≥ un nuevo √≥rden de las columnas colocando la columna PK en la primera columna.

`7. Eliminaci√≥n de columnas`

La columna ‚Äúrow_id‚Äù se elimin√≥ de las tablas donde ya se contaba con PK y donde no se contaba se constituy√≥ en la PK

`8. Se eliminan duplicados`

Se eliminan duplicados de las tablas que contaban con duplicados.


## **3. Carga**

El c√≥digo ETL creado transforma cada dataset en una sentencia T-SQL (INSERT INTO‚Ä¶) para agregar los datos a las tablas en SQL en un solo movimiento. Para lograrlo se ha usado la siguiente librer√≠a:

    import pymysql

`Creaci√≥n de Data Warehouse`

Como DBMS se utiliz√≥ MySQL workbench, todas las tablas fueron creadas utilizando T-SQL.

`Creaci√≥n manual de BD en MySQL`

La BD lleva por nombre pf_uci  y est√° creada dentro de Mysql Workbench.

`Creaci√≥n de tablas en MySQL`

La base de datos est√° conformada por 25 tablas. Cada tabla lleva los mismos nombres que los csv que fueron disponibilizados por el hospital. La tabla NOTEEVENTS no fue incluida en el armado de la base de datos ya que no es necesaria, se recomienda al cliente que deje de tomarla en cuenta o haga una modificaci√≥n en los datos que est√° recabando. Se renombraron las columnas con el nombre row_id, el nuevo formato de nombre para esas columnas es el nombre correspondiente de la tabla m√°s ( ‚Äú_id‚Äù) en las tablas de hechos. Esta columna fue removida de las tablas de dimensiones.

`Asignaci√≥n de PK y FK`

Una vez que los datos se han transformado, se pueden cargar en un destino deseado, como un archivo CSV o una base de datos. Las sentencias son construidas respetando las PK y FK, adem√°s de los formatos de las tablas.

`Automatizaci√≥n de tareas con AirFlow`

Finalmente, se ha automatizado el proceso ETL utilizando un programador de tareas o una herramienta de orquestaci√≥n como Apache Airflow para ejecutar el pipeline ETL en intervalos regulares.


  <br/>

# **VI. KPIs**
  
  <br/>

# **VII. MODELO DE MACHINE LEARNING**

<br/>

<p align=center><img src=https://github.com/espadaone/PF_UCI-/blob/c31c4bbdec1a6725a0daae09bac0b100a64608da/images/banner%20inferior.png><p>

<br/>
